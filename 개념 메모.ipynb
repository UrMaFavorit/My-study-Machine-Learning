{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 복원추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 복원추출- 데이터 중첩 허용\n",
    "- 중첩이 야기하는 효과\n",
    "-  다양한 알로리즘(중첩된 데이터에 집중한)이 나올수 있다\n",
    "-  결과적으로는 과대적합인데,여러개의 DT모델을 연결해서 \n",
    "-  평균, 투표를 통해서 결과를 집계하게 되면, 과대적합이 해소 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 랜덤포레스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DT가 여러개 모인것\n",
    "- 1.복원추출: 데이터 랜덤성\n",
    "- 2.Max feature: 특성 랜덤섬 -> DT에서 다양한 알고리즘을 갖을수 있음\n",
    "- 트리의 개수 :n_estimators\n",
    "- 선택할 특징dml 최대 수 : max_features -> 개별트리에 적용됨\n",
    "- 선택한 데이터의 시드 : random_state -> 랜덤성을 주는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #  GridSearch 를 효율적으로 사용하자\n",
    " - RandomForest모델에 학습을 시작\n",
    " - n_estimator의 최적의 값이 1000이고, 이것을 찾아보려고함\n",
    " - 1_2000까지 전부 돌려보고 최적의 값 찾기(가장 정확한 방법,가장 비효율적)\n",
    " - 1,500,1000,1500,2000 이런식으로 우선은 듬성듬성하게 돌림\n",
    " - 위의 학습에선 1000이 가장 좋은것으로 나옴(501-1499 사이에 최적의 파라미터 존재)\n",
    " - 다음은  600 800 1000 1200 1400 조금더 세세하게 학습\n",
    " - 1000이 가장 좋은것으로 나옴(801-1199 사이에 최적의 파라미터 존재)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 스케일링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 특성들의 범위를 정규화 해주는 작업\n",
    "- 특성마다 다른 범위를 가지는 경우 머신러닝 모델들이 제대로 학습되지 않을 가능성이 있다.\n",
    "-  거리기반으로 찾는 모델들의 경우 多\n",
    "    \n",
    "- 장점: \n",
    "-     특성들을 비교 분석하기 쉽게 만들어 준다.\n",
    "-     특성에 따라 서 원래 범위를 유지하는게 좋으면 굳이 스케일링을 할 필요가 없다.\n",
    "\n",
    "\n",
    "- 주의점: \n",
    "-     훈련세트와 테스트 세트에 같은 변환을 적용해야한다.\n",
    "\n",
    "\n",
    "\n",
    "- StandardScaler:\n",
    "-     모든 변수의 평균, 표준편차를 이용해 정규분포 형태로 변환(평균0, 분산1)\n",
    "-     이상치에 민감하게 영향을 받는다.\n",
    "   \n",
    "- RobustScaler:P.178하단\n",
    "-     변수의 사분위수(중간이상치를 제외)를 이요해 변환(75%,25%)\n",
    "-     이상치가 있는 데이터 변환 시 사용할수 있다.\n",
    "   \n",
    "- MinMaxScaler:\n",
    "-     변수의 Max,Min 값을 이용해서 변환(0-1사이 값으로 변환)\n",
    "-     이상치에 민감하게 영향을 받는다.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선형회귀모델\n",
    "\n",
    "- 입력특성(데이터)에 대한 선형함수를 만들어 예측을 수행\n",
    "- 다양한 선형모델 존재\n",
    "- 분류와 회귀 모두에 사용가능\n",
    "- --------------------------------------\n",
    "- 1개특성 1개 정답 =>  Wx+b\n",
    "- 2개특성 특성 => y= (W1*X1))+(W2*X2)+b\n",
    "- 특성이 p 개 => y= (W1*X1))+(W2*X2)....+(Wp*Xp)+b\n",
    "- w: 가중치, 기울기/ b:절편, 편향\n",
    "- --------------------------------------\n",
    "- 모든데이터를 아우를수 있는 함수를 만든다\n",
    "- 비용함수 : 수식을 검증 => 아우를수 있는가를 검증해주는함수\n",
    "- ↑오차들의 제곱값 사용\n",
    "- 예측함수: H(x)= W * X+ b    =>  H(x)-y : 예측값에서 실제값을 뺀것= 오차\n",
    "- 비용함수가 클수록 데이터 반영이 x\n",
    "- --------------------------------------\n",
    "\n",
    "- 평균제곱오차(비용함수)가 최소인 w(기울기) 와b(절편) 를 찾는다\n",
    "- 비용함수(오차)가 최소인 예측함수를 찾는것이 목표\n",
    "\n",
    "- --------------------------------------\n",
    "- 평균제곱오차(비용함수)\n",
    "- 비용함수=평균(예측값-실제값)제곱/ 오차들제곱의 평균\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <정리>\n",
    "# -----------------------------------\n",
    "\n",
    "# 선형회귀\n",
    "- 선형모델을 사용해서 회귀 분석\n",
    "\n",
    "# 선형모델\n",
    "-  입력특성에 대해서 입력틍성을 모두 반영하는 예츨함수 (선x)을 그어서 예측\n",
    "- 예측함수 특성의 개수만큼의 차원을 가짐\n",
    "\n",
    "# 예측함수\n",
    "- 예측함수가 제대로 예측한 함수인지 어떻게 구분할 것인가?\n",
    "- 비용함수(함수를검증하는 함수): 평균제곱오차(MSE)\n",
    "- => 평균제곱 오차가 가장 작은 예측함수가 입력특성을 모두 반영하는 예측함수\n",
    "\n",
    "#  어떻게 평균제곱오차가 가장 작은 예측함수를 찾을것인가?\n",
    "- 수학적 공식 > 수학적 공식을 통해서 평균제곱오차가 가장작은 예측함수를 한번 에 찾음\n",
    "- 경사하강법 > 임이의 한점에서부터 평균제곱오차가 가장 작은 점을 서서히 찾아감\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  경사하강법\n",
    "- x축:가중치/y축: 비용함수(평균제곱오차의 값)\n",
    "- 비용함수(U자 그래프)의 기울기(경사)를 구하여 기울기가 낮은쪽으로 계속 이동하여 값을 최적화 시키는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  LinearRegression모델\n",
    "- 수학적 공식을 이용한 모델\n",
    "- 비용함수(평균제곱오차)가 최소가 되는 지점을 계산을 통해서 한번에 출력\n",
    "- 출력값을 수정할 방법이 없기 때문데 과대. 과소 적합을 해소 할수 없다.\n",
    "\n",
    "#  LinearRegression + 정규화(규제) > 과대 /과소적합해소\n",
    "- Lasso\n",
    "- Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE와 RMSE는 값의 범위가 무한정 늘어나기 떄문에 좋ㅎ은값과 나쁜값의 구분이 힘듬\n",
    "- 따라서 평가 지표고 R^2를 사용\n",
    "- 값의 범위가  0 ~ 1\n",
    "- 1로 나올수록 좋은 평가지표= 확률값으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분류평가지표\n",
    "## 재현율\n",
    "- 재현율(recall):실제양성중에 예측양성 비율 \n",
    "- 실제 양성 :TP+FN / 예츨한 양성중에 맞춘것 TP\n",
    "\n",
    "##  정밀도\n",
    "- 정밀도(precision):p라고 예측한 것중에서 정말로 P인것\n",
    "\n",
    "##  F1-socre\n",
    "- 정밀도와 재현율이 적당하게 있느냐 /1에 가까울수록 좋은 것\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
